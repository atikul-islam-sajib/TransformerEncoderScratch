{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import yaml\n",
    "import torch\n",
    "import joblib\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomException(Exception):\n",
    "    def __init__(self, message: str):\n",
    "        super(CustomException, self).__init__()\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "def dump(value: str, filename: str):\n",
    "    if (value is not None) and (filename is not None):\n",
    "        joblib.dump(value=value, filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise CustomException(\"Cannot be dump into pickle file\".capitalize())\n",
    "\n",
    "\n",
    "def load(filename: str):\n",
    "    if filename is not None:\n",
    "        joblib.load(filename=filename)\n",
    "\n",
    "    else:\n",
    "        raise CustomException(\"Cannot be load the pickle file\".capitalize())\n",
    "\n",
    "\n",
    "def device_init(self, device: str = \"mps\"):\n",
    "    if device == \"cuda\":\n",
    "        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    elif device == \"mps\":\n",
    "        return torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def config():\n",
    "    with open(\"../config.yml\", \"r\") as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled_dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    dimension: int = 512,\n",
    "    mask=None,\n",
    "):\n",
    "    if (\n",
    "        (isinstance(query, torch.Tensor))\n",
    "        and (isinstance(key, torch.Tensor))\n",
    "        and isinstance(values, torch.Tensor)\n",
    "    ):\n",
    "        result = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(dimension)\n",
    "\n",
    "        if (mask is not None) and isinstance(mask, torch.Tensor):\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "            result = torch.add(result, mask)\n",
    "\n",
    "        result = torch.softmax(input=result, dim=-1)\n",
    "\n",
    "        attention = torch.matmul(result, values)\n",
    "\n",
    "        return result, attention\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\n",
    "            \"All inputs, for instance, query, key, and values must be of type torch.Tensor\".capitalize()\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = config()[\"embedding\"][\"batch_size\"]\n",
    "    sequence_length = config()[\"embedding\"][\"sequence_length\"]\n",
    "\n",
    "    dimension = config()[\"transformer\"][\"dimension\"]\n",
    "    heads = config()[\"transformer\"][\"heads\"]\n",
    "\n",
    "    query = key = values = torch.randn(\n",
    "        (\n",
    "            batch_size,\n",
    "            heads,\n",
    "            sequence_length,\n",
    "            dimension // heads,\n",
    "        )\n",
    "    )\n",
    "    mask = torch.randn((batch_size, sequence_length))\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Scaled Dot Product Attention for Transformers\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"dimension\"],\n",
    "        help=\"Dimension of the input\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--query\",\n",
    "        type=torch.Tensor,\n",
    "        default=query,\n",
    "        help=\"Query tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--key\",\n",
    "        type=torch.Tensor,\n",
    "        default=key,\n",
    "        help=\"Key tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--values\",\n",
    "        type=torch.Tensor,\n",
    "        default=values,\n",
    "        help=\"Values tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mask\",\n",
    "        type=torch.Tensor,\n",
    "        default=mask,\n",
    "        help=\"Mask tensor for padding\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    values, attention = scaled_dot_product(\n",
    "        query=args.query,\n",
    "        key=args.key,\n",
    "        values=args.values,\n",
    "        mask=args.mask,\n",
    "    )\n",
    "\n",
    "    assert args.query.size() == attention.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multihead-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttenion(nn.Module):\n",
    "    def __init__(self, dimension: int = 512, heads: int = 8, mask=None):\n",
    "        super(MultiHeadAttenion, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "\n",
    "        assert (\n",
    "            self.dimension % self.heads == 0\n",
    "        ), \"dimension should be divisible by heads\".capitalize()\n",
    "\n",
    "        self.QKV = nn.Linear(\n",
    "            in_features=self.dimension, out_features=3 * self.dimension, bias=False\n",
    "        )\n",
    "        self.layer = nn.Linear(\n",
    "            in_features=self.dimension, out_features=self.dimension, bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.QueryKeyValues = self.QKV(x)\n",
    "\n",
    "            self.query, self.key, self.values = torch.chunk(\n",
    "                self.QueryKeyValues, 3, dim=-1\n",
    "            )\n",
    "\n",
    "            self.query = self.query.view(\n",
    "                self.query.size(0),\n",
    "                self.query.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "            self.key = self.key.view(\n",
    "                self.key.size(0),\n",
    "                self.key.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "            self.values = self.values.view(\n",
    "                self.values.size(0),\n",
    "                self.values.size(1),\n",
    "                self.heads,\n",
    "                self.dimension // self.heads,\n",
    "            )\n",
    "\n",
    "            self.query = self.query.permute(0, 2, 1, 3)\n",
    "            self.key = self.key.permute(0, 2, 1, 3)\n",
    "            self.values = self.values.permute(0, 2, 1, 3)\n",
    "\n",
    "            _, attention = scaled_dot_product(\n",
    "                query=self.query, key=self.key, values=self.values, mask=self.mask\n",
    "            )\n",
    "\n",
    "            assert (\n",
    "                attention.size() == self.query.size()\n",
    "                and self.key.size()\n",
    "                and self.values.size()\n",
    "            ), \"attention size is not equal to query, key and values size\".capitalize()\n",
    "\n",
    "            self.attention = attention.view(\n",
    "                attention.size(0),\n",
    "                attention.size(2),\n",
    "                attention.size(1),\n",
    "                attention.size(3),\n",
    "            )\n",
    "\n",
    "            self.attention = self.attention.view(\n",
    "                self.attention.size(0), self.attention.size(1), -1\n",
    "            )\n",
    "\n",
    "            assert (\n",
    "                self.attention.size(-1) == self.dimension\n",
    "            ), \"attention size is not equal to dimension\".capitalize()\n",
    "\n",
    "            return self.layer(self.attention)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"MultiHeadAttention for Transformer Encoder\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dimension\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"dimension\"],\n",
    "        help=\"dimension of the input\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--heads\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"heads\"],\n",
    "        help=\"number of heads\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mask\",\n",
    "        type=torch.Tensor,\n",
    "        default=None,\n",
    "        help=\"mask for attention\".capitalize(),\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    batch_size = config()[\"embedding\"][\"batch_size\"]\n",
    "    sequence_length = config()[\"embedding\"][\"sequence_length\"]\n",
    "\n",
    "    attention = MultiHeadAttenion(\n",
    "        dimension=args.dimension, heads=args.heads, mask=args.mask\n",
    "    )\n",
    "\n",
    "    assert attention(\n",
    "        torch.randn(batch_size, sequence_length, args.dimension)\n",
    "    ).size() == (\n",
    "        batch_size,\n",
    "        sequence_length,\n",
    "        args.dimension,\n",
    "    ), \"MultiHeadAttention output size is not equal to input size\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_shape: int = 512, epsilon: float = 1e-05):\n",
    "        super(LayerNormalization, self).__init__()\n",
    "\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.ones((self.normalized_shape,)))\n",
    "        self.betas = nn.Parameter(torch.zeros((self.normalized_shape,)))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            self.mean = x.mean(dim=-1)\n",
    "            self.variance = x.var(dim=-1, unbiased=False)\n",
    "\n",
    "            self.mean = self.mean.unsqueeze(-1)\n",
    "            self.variance = self.variance.unsqueeze(-1)\n",
    "\n",
    "            return (\n",
    "                self.gamma\n",
    "                * ((x - self.mean) / (torch.sqrt(self.variance + self.epsilon)))\n",
    "                + self.betas\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Layer Normalization for Transformer encoder\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--normalized_shape\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"dimension\"],\n",
    "        help=\"The normalized shape of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon\",\n",
    "        type=float,\n",
    "        default=config()[\"transformer\"][\"eps\"],\n",
    "        help=\"Epsilon value\".capitalize(),\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    normalization = LayerNormalization(\n",
    "        normalized_shape=args.normalized_shape, epsilon=args.epsilon\n",
    "    )\n",
    "\n",
    "    assert normalization(\n",
    "        torch.randn(\n",
    "            config()[\"embedding\"][\"batch_size\"],\n",
    "            config()[\"embedding\"][\"sequence_length\"],\n",
    "            args.normalized_shape,\n",
    "        )\n",
    "    ).size() == (\n",
    "        config()[\"embedding\"][\"batch_size\"],\n",
    "        config()[\"embedding\"][\"sequence_length\"],\n",
    "        args.normalized_shape,\n",
    "    ), \"Dimension mismatch in the layer normalization layer\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Feed Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseNeuralNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features: int = 512, out_features: int = 512, dropout: float = 0.1\n",
    "    ):\n",
    "        super(PointWiseNeuralNetwork, self).__init__()\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        for index in range(2):\n",
    "            self.layers.append(\n",
    "                nn.Linear(in_features=self.in_features, out_features=self.out_features),\n",
    "            )\n",
    "\n",
    "            self.in_features = self.out_features\n",
    "            self.out_features = in_features\n",
    "\n",
    "            if index % 2 == 0:\n",
    "                self.layers.append(nn.ReLU(inplace=True))\n",
    "                self.layers.append(nn.Dropout(p=self.dropout))\n",
    "\n",
    "        self.layer = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.layer(x)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Pointwise neural network for transformer\".title()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--in_features\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"dimension\"],\n",
    "        help=\"Input features\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--out_features\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"feed_forward\"],\n",
    "        help=\"Output features\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\", type=float, default=0.1, help=\"Dropout rate\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    net = PointWiseNeuralNetwork(\n",
    "        in_features=args.in_features,\n",
    "        out_features=args.out_features,\n",
    "        dropout=args.dropout,\n",
    "    )\n",
    "\n",
    "    batch_size = config()[\"embedding\"][\"batch_size\"]\n",
    "    sequence_length = config()[\"embedding\"][\"sequence_length\"]\n",
    "\n",
    "    assert (\n",
    "        args.in_features % config()[\"transformer\"][\"heads\"] == 0\n",
    "    ), \"Input features must be divisible by the number of heads\".capitalize()\n",
    "\n",
    "    assert net(torch.rand((batch_size, sequence_length, args.in_features))).size() == (\n",
    "        batch_size,\n",
    "        sequence_length,\n",
    "        args.in_features,\n",
    "    ), \"Dimensions do not match in the poinytwise neural network\".capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        heads: int = 8,\n",
    "        feed_forward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        epsilon: float = 1e-5,\n",
    "        mask=None,\n",
    "    ):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.haeds = heads\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.mask = mask\n",
    "\n",
    "        self.multihead_attention = MultiHeadAttenion(\n",
    "            dimension=self.dimension, heads=self.haeds, mask=self.mask\n",
    "        )\n",
    "\n",
    "        self.layer_norm = LayerNormalization(\n",
    "            normalized_shape=self.dimension, epsilon=self.epsilon\n",
    "        )\n",
    "\n",
    "        self.feedfoward = PointWiseNeuralNetwork(\n",
    "            in_features=self.dimension,\n",
    "            out_features=self.feed_forward,\n",
    "            dropout=self.dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            residual = x\n",
    "\n",
    "            x = self.multihead_attention(x)\n",
    "            x = torch.dropout(input=x, p=self.dropout, train=self.training)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            x = self.feedfoward(residual)\n",
    "            x = torch.dropout(input=x, p=self.dropout, train=self.training)\n",
    "            x = torch.add(x, residual)\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a torch.Tensor\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Encoder Block for transformers\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--d_model\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"dimension\"],\n",
    "        help=\"Dimension of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--heads\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"heads\"],\n",
    "        help=\"Number of heads in the multihead attention\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feedforward\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"feed_forward\"],\n",
    "        help=\"Dimension of the feedforward layer\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=config()[\"transformer\"][\"dropout\"],\n",
    "        help=\"Dropout rate\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon\",\n",
    "        type=float,\n",
    "        default=config()[\"transformer\"][\"eps\"],\n",
    "        help=\"Epsilon for layer norm\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mask\",\n",
    "        type=torch.Tensor,\n",
    "        default=None,\n",
    "        help=\"Mask for attention\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--display\", action=\"store_true\", help=\"Display the arguments\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dimension = args.d_model\n",
    "    heads = args.heads\n",
    "    feed_forward = args.feedforward\n",
    "    dropout = args.dropout\n",
    "    epsilon = args.epsilon\n",
    "    mask = args.mask\n",
    "\n",
    "    batch_size = config()[\"embedding\"][\"batch_size\"]\n",
    "    sequence_length = config()[\"embedding\"][\"sequence_length\"]\n",
    "\n",
    "    encoder = EncoderBlock(\n",
    "        dimension=dimension,\n",
    "        heads=heads,\n",
    "        feed_forward=feed_forward,\n",
    "        dropout=dropout,\n",
    "        epsilon=epsilon,\n",
    "        mask=mask,\n",
    "    )\n",
    "\n",
    "    assert encoder(torch.randn(batch_size, sequence_length, dimension)).size() == (\n",
    "        batch_size,\n",
    "        sequence_length,\n",
    "        dimension,\n",
    "    ), \"Dimension mismatch in the EncoderBlock\".capitalize()\n",
    "\n",
    "    if args.display:\n",
    "        print(summary(model=encoder, input_size=(sequence_length, dimension)), \"\\n\")\n",
    "\n",
    "        draw_graph(\n",
    "            model=encoder,\n",
    "            input_data=torch.randn(batch_size, sequence_length, dimension),\n",
    "        ).visual_graph.render(\n",
    "            filename=os.path.join(config()[\"path\"][\"FILES_PATH\"], \"encoder\"),\n",
    "            format=\"png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimension: int = 512,\n",
    "        heads: int = 8,\n",
    "        feed_forward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "        epsilon: float = 1e-5,\n",
    "        mask=None,\n",
    "    ):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.heads = heads\n",
    "        self.feed_forward = feed_forward\n",
    "        self.dropout = dropout\n",
    "        self.epsilon = epsilon\n",
    "        self.mask = mask\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *[\n",
    "                EncoderBlock(\n",
    "                    dimension=self.dimension,\n",
    "                    heads=self.heads,\n",
    "                    feed_forward=self.feed_forward,\n",
    "                    dropout=self.dropout,\n",
    "                    epsilon=self.epsilon,\n",
    "                    mask=self.mask,\n",
    "                )\n",
    "                for _ in range(self.heads)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            return self.model(x)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a tensor\".capitalize())\n",
    "\n",
    "    @staticmethod\n",
    "    def total_params(model=None):\n",
    "        if isinstance(model, TransformerEncoder):\n",
    "            return sum(params.numel() for params in model.parameters())\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Input must be a transformer encoder\".capitalize())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Encoder Block for transformers\".capitalize()\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--d_model\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"dimension\"],\n",
    "        help=\"Dimension of the input tensor\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--heads\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"heads\"],\n",
    "        help=\"Number of heads in the multihead attention\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--feedforward\",\n",
    "        type=int,\n",
    "        default=config()[\"transformer\"][\"feed_forward\"],\n",
    "        help=\"Dimension of the feedforward layer\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dropout\",\n",
    "        type=float,\n",
    "        default=config()[\"transformer\"][\"dropout\"],\n",
    "        help=\"Dropout rate\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--epsilon\",\n",
    "        type=float,\n",
    "        default=config()[\"transformer\"][\"eps\"],\n",
    "        help=\"Epsilon for layer norm\".capitalize(),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mask\",\n",
    "        type=torch.Tensor,\n",
    "        default=None,\n",
    "        help=\"Mask for attention\".capitalize(),\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--display\", action=\"store_true\", help=\"Display the arguments\".capitalize()\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dimension = args.d_model\n",
    "    heads = args.heads\n",
    "    feed_forward = args.feedforward\n",
    "    dropout = args.dropout\n",
    "    epsilon = args.epsilon\n",
    "    mask = args.mask\n",
    "\n",
    "    batch_size = config()[\"embedding\"][\"batch_size\"]\n",
    "    sequence_length = config()[\"embedding\"][\"sequence_length\"]\n",
    "\n",
    "    netTransfomer = TransformerEncoder(\n",
    "        dimension=dimension,\n",
    "        heads=heads,\n",
    "        feed_forward=feed_forward,\n",
    "        dropout=dropout,\n",
    "        epsilon=epsilon,\n",
    "        mask=mask,\n",
    "    )\n",
    "    assert netTransfomer(\n",
    "        torch.randn(batch_size, sequence_length, dimension)\n",
    "    ).size() == (\n",
    "        batch_size,\n",
    "        sequence_length,\n",
    "        dimension,\n",
    "    ), \"Dimension mismatch in the EncoderBlock\".capitalize()\n",
    "\n",
    "    if args.display:\n",
    "\n",
    "        print(\n",
    "            \"Total parameters of the model is # {}\".format(\n",
    "                TransformerEncoder.total_params(model=netTransfomer)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(summary(model=netTransfomer, input_size=(sequence_length, dimension)))\n",
    "\n",
    "        draw_graph(\n",
    "            model=netTransfomer,\n",
    "            input_data=torch.randn(batch_size, sequence_length, dimension),\n",
    "        ).visual_graph.render(\n",
    "            filename=os.path.join(config()[\"path\"][\"FILES_PATH\"], \"transfomerEncoder\"),\n",
    "            format=\"png\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from .transformer import TransformerEncoder\n",
    "\n",
    "\"\"\"\n",
    "This script initializes a Transformer Encoder with specified parameters, \n",
    "creates a random embedding tensor, and prints the shapes of the embedding \n",
    "and the output tensors.\n",
    "\n",
    "Attributes:\n",
    "    batch_size (int): The batch size for the input tensor.\n",
    "    sequence_length (int): The sequence length for the input tensor.\n",
    "    model_dimension (int): The dimension of the model.\n",
    "    feed_forward (int): The dimension of the feed forward network.\n",
    "    number_heads (int): The number of attention heads.\n",
    "    dropout (float): The dropout rate.\n",
    "    epsilon (float): The epsilon value for numerical stability.\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 64\n",
    "sequence_length = 512\n",
    "model_dimension = 768\n",
    "feed_forward = 2048\n",
    "number_heads = 12\n",
    "dropout = 0.1\n",
    "epsilon = 1e-6\n",
    "\n",
    "# Create a random embedding tensor with the specified shape\n",
    "embedding = torch.randn((batch_size, sequence_length, model_dimension))\n",
    "\n",
    "# Create a random padding mask tensor\n",
    "padding_masked = torch.randn((batch_size, sequence_length))\n",
    "\n",
    "# Initialize the Transformer Encoder with the specified parameters\n",
    "netTransformer = TransformerEncoder(\n",
    "    dimension=model_dimension,\n",
    "    heads=number_heads,\n",
    "    feed_forward=feed_forward,\n",
    "    dropout=dropout,\n",
    "    epsilon=epsilon,\n",
    "    mask=padding_masked,\n",
    ")\n",
    "\n",
    "# Print the divider line\n",
    "print(\"|\", \"-\" * 100, \"|\")\n",
    "\n",
    "# Print the shape of the embedding tensor\n",
    "print(\"|\", \"\\tThe embedding shape is: \", embedding.size())\n",
    "\n",
    "# Pass the embedding through the Transformer Encoder and print the output shape\n",
    "print(\n",
    "    \"|\",\n",
    "    \"\\tThe output shape is: \",\n",
    "    netTransformer(embedding).size(),\n",
    ")  # (batch_size, sequence_length, model_dimension)\n",
    "\n",
    "# Print the closing divider line\n",
    "print(\"|\", \"-\" * 100, \"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPSG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
